<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Climate penalty</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="../assets/img/favicon.png" rel="icon">
  <link href="../assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Roboto:300,300i,400,400i,500,500i,700,700i&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="../assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="../assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="../assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="../assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Moderna - v4.11.0
  * Template URL: https://bootstrapmade.com/free-bootstrap-template-corporate-moderna/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top ">
    <div class="container d-flex align-items-center justify-content-lg-between">

      <h1 class="text-light"><a href="../index.html"><span style="color: white; font-size: 25px;">Avinya</span></a></h1>
      <!-- Uncomment below if you prefer to use an image logo -->
      <!-- <a href="index.html" class="logo me-auto me-lg-0"><img src="../assets/img/logo.png" alt="" class="img-fluid"></a>-->

      <nav id="navbar" class="navbar">
        <ul>
          <li><a href="../index.html">Home</a></li>
          <li><a href="EDA.html">EDA</a></li>
          <li><a href="Clustering.html">Clustering</a></li>
          <li><a  href="ARM.html">ARM</a></li>
          <li><a class="active "  href="DT.html">Decision Tree</a></li>
          <li><a href="NB.html">Naive Bayes</a></li>
          <li><a href="SVM.html">SVM</a></li>

        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->
      

    </div>
  </header><!-- End Header -->




  
  <main id="main">
     <!-- ======= About Us Section ======= -->
     <section class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Decision Tree</h2>
          <ol>
            <li><a href="../index.html">Home</a></li>
            <li>Decision Tree</li>
          </ol>
        </div>

      </div>
    </section><!-- End About Us Section -->
   
    <!-- ======= Features Section ======= -->
   <!-- 
  
  Radio version of tabs.

  Requirements:
  - not rely on specific IDs for CSS (the CSS shouldn't need to know specific IDs)
  - flexible for any number of unkown tabs [2-6]
  - accessible

  Caveats:
  - since these are checkboxes the tabs not tab-able, need to use arrow keys

  Also worth reading:
  http://simplyaccessible.com/article/danger-aria-tabs/
-->

<div class="tabset">
  <!-- Tab 1 -->
  <input type="radio" name="tabset" id="tab1" aria-controls="marzen" checked>
  <label for="tab1">Introduction</label>
  <!-- Tab 2 -->
  <input type="radio" name="tabset" id="tab2" aria-controls="rauchbier">
  <label for="tab2">Data Gathering</label>
  <!-- Tab 3 -->
  <input type="radio" name="tabset" id="tab3" aria-controls="dunkles">
  <label for="tab3">Results & Conclusion </label>
  
  <div class="tab-panels">
    <section id="marzen" class="tab-panel">
      
        <div class="container" style="max-width: 100%;">
  
          <div class="row" >
            <div class="col-md-5">
              <img src="../assets/img/DT/dt2.png" class="img-fluid" alt="" style="height: 270px; width: 700px;">
            </div>
            <div class="col-md-7">
              <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
                A decision tree is a machine learning algorithm that can be used for both classification and regression tasks. It is a type of model that makes predictions by learning simple decision rules from data. A decision tree can be seen as a flowchart-like structure, where each internal node represents a "test" on a feature or attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a numerical value that is the result of the decision tree's prediction.

                Decision trees are often used in data mining and predictive analytics applications, as they are easy to interpret and can be used for both categorical and numerical data. They can also handle both continuous and discrete variables, making them very versatile.
                
                </p>
             
            </div>
            <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
              Some common applications of decision trees include:
              
              Customer segmentation: Decision trees can be used to group customers based on their behavior, preferences, and demographics, allowing businesses to tailor their marketing and product offerings to specific customer segments.
              
              Fraud detection: Decision trees can be used to detect fraudulent transactions by identifying patterns in data that indicate fraud.
              
              Medical diagnosis: Decision trees can be used to diagnose medical conditions based on symptoms and patient history.
              
              Predictive maintenance: Decision trees can be used to predict when equipment or machinery is likely to fail, allowing maintenance teams to proactively schedule repairs and prevent downtime.
              
              Overall, decision trees are a powerful tool for making predictions based on complex data sets, and they have a wide range of applications across industries.
    
              
            </p>
          </div>
  
          <div class="row" data-aos="fade-up">
            <div class="col-md-5 order-1 order-md-2">
              <img src="../assets/img/DT/dt1.webp" class="img-fluid" alt="" style="height: 200px">
              <br/><br/>
              <img src="../assets/img/DT/dt3.jpeg" class="img-fluid" alt="" style="height: 300px">
            </div>
            <div class="col-md-7 pt-5 order-2 order-md-1">
              
            
              <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
                When building decision trees, we need a way to determine which features or attributes to split on, and which ones are the most informative for making predictions. GINI, Entropy, and Information Gain are three popular measures used to evaluate the quality of a split in a decision tree.

GINI Impurity: GINI is a measure of impurity or randomness used to evaluate the quality of a binary split. It calculates the probability of misclassification if a random sample is classified according to the distribution of target classes in the split. A GINI score of 0 means that all samples in a node belong to the same class, while a score of 0.5 means that the samples are equally distributed among all classes.

Entropy: Entropy is another measure of impurity or randomness used to evaluate the quality of a binary split. It calculates the amount of uncertainty or disorder in a system, which in this case, is the distribution of target classes in the split. A split with low entropy means that the target classes are mostly the same, while a split with high entropy means that the target classes are distributed more evenly.

Information Gain: Information Gain is a measure of how much information is gained by splitting on a particular feature or attribute. It calculates the difference in entropy or GINI impurity between the parent node and the child nodes resulting from the split. A higher information gain means that the split is more informative and should be preferred over other splits.
              </p>
            </div>
            <hr/>
          
         
  
        </div>
      </section><!-- End Features Section -->
    <section id="rauchbier" class="tab-panel">
     <ol>
      <li style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">Used Clustering data: ClusterData_Twitter <br/>
        Link to the dataset: <a href="">Dataset</a>
        <br/> Link to the Python code: <a href="">Code</a>

        <br/>
      <div class="row">
        <div class="col-lg-6">
          <img src="../assets/img/train.jpg" class="img-fluid" alt="" style="height: 350px;width: 100%;">
         
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0">
           
            <img src="../assets/img/test.jpg" class="img-fluid" alt="" style="height: 300px">
        </div> 
          <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
            
            The dataset include numeric values that is pollutants emissions of Carbondioxide, Methane, FGas and Nitrogen oxide with the additional column
            called Risk factor rated from 1 to 3 based on the emissions from the pollutants
          </p>
     <br/>

    
    </ol>
    </section>
    <section id="dunkles" class="tab-panel">
      <h4>Results</h4>
      <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
        </p>

      <div class="row">
        <b></b>

        <div class="col-lg-6">
          <img src="../assets/img/heatmap.png" class="img-fluid" alt="" >
    
        </div>

        <div class="col-lg-6 pt-4 pt-lg-0">
          <img src="../assets/img/confmat.jpg" class="img-fluid" alt="" >
      </div>
     <br/>
      <hr/>

     
      <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
       
      </p>
      <div class="row">
        <b></b>
        <div class="col-lg-6">
          <img src="../assets/img/DT/gini2.png" class="img-fluid" alt="">
    
        </div>

        <div class="col-lg-6 pt-4 pt-lg-0">
          <img src="../assets/img/DT/gini.png" class="img-fluid" alt="" >
      </div>
      <p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
        Here is a brief explanation of the arguments passed to the DecisionTreeClassifier constructor:

        criterion: The function to measure the quality of a split. "entropy" is a measure of impurity based on the information theory, while "gini" is a measure based on the Gini impurity.
        
        splitter: The strategy used to choose the split at each node. "random" means that random splits are tried, while "best" means that the best split is chosen.
        
        max_depth: The maximum depth of the decision tree.
        
        min_samples_split: The minimum number of samples required to split an internal node.
        
        min_samples_leaf: The minimum number of samples required to be at a leaf node.
        
        min_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.
        
        max_features: The maximum number of features to consider when looking for the best split.
        
        random_state: The seed used by the random number generator.
        
        max_leaf_nodes: The maximum number of leaf nodes in the decision tree.
        
        min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.
        
        class_weight: Weights associated with classes in the form {class_label: weight}.
        
        By setting these arguments to specific values, the user can customize the behavior of the decision tree algorithm. Once the instance of DecisionTreeClassifier is created, it can be trained on a labeled dataset using the fit() method and used to predict the class labels of new data points using the predict() method.

   </p>
   <div class="col-lg-6">
    <img src="../assets/img/dt1_cm.png" class="img-fluid" alt="" >

  </div>

  <div class="col-lg-6 pt-4 pt-lg-0">
    <img src="../assets/img/dt1.png" class="img-fluid" alt="" >
</div>
<br/>
<hr/>
<p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
  It's important to note that the choice of criterion and splitter can have a significant impact on the performance of the decision tree algorithm. For example, "gini" is typically used for smaller datasets, while "entropy" may be better for larger datasets with more features. Similarly, "best" is usually a better choice than "random" for most datasets.

In practice, it's often a good idea to try out different combinations of hyperparameters and evaluate their performance on a validation set before selecting the best model. The 
</p>
<div class="col-lg-6">
  <img src="../assets/img/dt2_cm.png" class="img-fluid" alt="" >

</div>

<div class="col-lg-6 pt-4 pt-lg-0">
  <img src="../assets/img/dt2 2.png" class="img-fluid" alt="" >
</div>
<br/>
<hr/>
<p style="font-size: 20px;font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif, Courier, monospace; padding: 10px;">
  By increasing the value of min_samples_split, the decision tree will have fewer nodes and may become less complex. This can help to avoid overfitting, especially when dealing with small datasets or noisy data. However, setting it too high may result in underfitting, where the decision tree is too simple and cannot capture the underlying patterns in the data. Therefore, it's important to carefully choose this hyperparameter based on the specific problem and the characteristics of the dataset.
</p>
<div class="col-lg-6">
  <img src="../assets/img/dt3.png" class="img-fluid" alt="" >

</div>

<div class="col-lg-6 pt-4 pt-lg-0">
  <img src="../assets/img/dt3v3.png" class="img-fluid" alt="" >
</div>
<br/>
<hr/>
  </div>
  
  </section>
</div>



    
  

  </main><!-- End #main -->

 

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="../assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="../assets/vendor/aos/aos.js"></script>
  <script src="../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="../assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="../assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="../assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="../assets/js/main.js"></script>

</body>

</html>